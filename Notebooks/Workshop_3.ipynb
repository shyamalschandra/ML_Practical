{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workshop3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yUOpuIX4ZMA"
      },
      "source": [
        "In this workshop, we get our hands dirty with neural networks! While implementing a neural network from scratch is not an easy task (check out ACM AI's advanced track!), Python libraries like Scikit make it easy to implement simple neural networks. Eventually, its probably a good idea to implement deep learning models with Tensor Flow or Pytorch, but Scikit is a pretty good place to start!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rn9ndQQ-4SrM",
        "outputId": "f3ab364e-65ec-4345-f6f7-d084bea6aa7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/blood-transfusion/transfusion.data')\n",
        "print(data.info())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 748 entries, 0 to 747\n",
            "Data columns (total 5 columns):\n",
            " #   Column                                      Non-Null Count  Dtype\n",
            "---  ------                                      --------------  -----\n",
            " 0   Recency (months)                            748 non-null    int64\n",
            " 1   Frequency (times)                           748 non-null    int64\n",
            " 2   Monetary (c.c. blood)                       748 non-null    int64\n",
            " 3   Time (months)                               748 non-null    int64\n",
            " 4   whether he/she donated blood in March 2007  748 non-null    int64\n",
            "dtypes: int64(5)\n",
            "memory usage: 29.3 KB\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy2W0JimHgaa",
        "outputId": "38b50ec8-dd5f-4f00-a1ff-12504db1050a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data = data.to_numpy()\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[    2    50 12500    98     1]\n",
            " [    0    13  3250    28     1]\n",
            " [    1    16  4000    35     1]\n",
            " ...\n",
            " [   23     3   750    62     0]\n",
            " [   39     1   250    39     0]\n",
            " [   72     1   250    72     0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "np0x0k9_G7yP",
        "outputId": "256e400a-b56e-4847-f8cd-5d269de0c83c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "# Exercise: Shuffle the data!\n",
        "np.random.shuffle(data)\n",
        "print(data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[   4    4 1000   43    1]\n",
            " [   4   10 2500   28    1]\n",
            " [   2    1  250    2    0]\n",
            " ...\n",
            " [   4   17 4250   71    1]\n",
            " [   8   10 2500   63    0]\n",
            " [   4    6 1500   16    1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOZljpEsI1Ke",
        "outputId": "4aab4ff9-513b-4a1f-c527-565586942af3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Print the seventh row of data\n",
        "# print(data[6])\n",
        "# Print the second column of the data\n",
        "# print(data[:,1])\n",
        "# Print the last column of the data\n",
        "# print(data[:,-1])\n",
        "# Print the 3rd element of the fifth row\n",
        "# print(data[4][2])\n",
        "\n",
        "# Print all columns except the first two\n",
        "data[:,2:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1000,   43,    1],\n",
              "       [2500,   28,    1],\n",
              "       [ 250,    2,    0],\n",
              "       ...,\n",
              "       [4250,   71,    1],\n",
              "       [2500,   63,    0],\n",
              "       [1500,   16,    1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnenxoq9Iqwc"
      },
      "source": [
        "split1 = int(0.6*len(data))\n",
        "split2 = int(0.8*len(data))\n",
        "# Split the data into training, cross validation and testing data\n",
        "train, cv, test = data[:split1,:], data[split1:split2,:], data[split2:,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGJ_BIx5KKdR"
      },
      "source": [
        "# Exercise: Split train, cv and test into X and Y\n",
        "trainX = train[:,:4] #train[:,:-1]\n",
        "trainY = train[:,-1] #train[:,4]\n",
        "\n",
        "cvX = cv[:,:4]\n",
        "cvY = cv[:,-1] #cv[:,4]\n",
        "\n",
        "testX = test[:,:4]\n",
        "testY = test[:,-1] #test[:,4]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0TCwGDy8F9e"
      },
      "source": [
        "Before implementing any ML model, its always a good idea to standardize our data. This means that for every feature, we subtract the mean and divide by the standard deiviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVbYa9MYAd6k",
        "outputId": "06e52ea1-992e-48d4-b807-6f73f5b26be4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Clearly, we do not have zero mean and unit variance\n",
        "print(trainX.var(axis=0))\n",
        "print(trainX.mean(axis=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[5.43112046e+01 3.39849928e+01 2.12406205e+06 5.75997748e+02]\n",
            "[   9.07589286    5.54017857 1385.04464286   34.18303571]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4TSdJLSAs49"
      },
      "source": [
        "Let Python do the job of scaling for us!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWk7kIYh77uT",
        "outputId": "6e846859-c864-4a12-e50d-06411c2c3030",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "# Scale the training data!\n",
        "scaler.fit(trainX)\n",
        "trainX = scaler.transform(trainX)\n",
        "\n",
        "print(\"Variance: \", trainX.var(axis=0)) #every variance should be 1\n",
        "print(\"Mean: \", trainX.mean(axis=0)) #every mean should be 0 or very close to 0"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variance:  [1. 1. 1. 1.]\n",
            "Mean:  [-4.36159045e-17  6.34413157e-17 -2.77555756e-17 -6.34413157e-17]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgfiLpj9FNTD"
      },
      "source": [
        "We also scale the cross validation and testing data according to the mean and variance of the training data. \n",
        "(Discuss: Why don't we scale the cross validation data and testing data with their own mean and variance?)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OGMHZZBNip8",
        "outputId": "6664842b-e3be-4864-ef72-ac3ace98ee9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"Variance of cv: \", cvX.var(axis=0))\n",
        "print(\"Mean of cv: \", cvX.mean(axis=0))\n",
        "print(\"Variance of test: \", testX.var(axis=0))\n",
        "print(\"Mean of test: \", testX.mean(axis=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variance of cv:  [1.11711156e+02 2.58862667e+01 1.61789167e+06 5.32470400e+02]\n",
            "Mean of cv:  [  10.71333333    5.02       1255.           32.64      ]\n",
            "Variance of test:  [5.04291556e+01 4.19955556e+01 2.62472222e+06 6.99971600e+02]\n",
            "Mean of test:  [   9.58666667    5.93333333 1483.33333333   36.22      ]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxJd-DIGFdmG",
        "outputId": "e7f0b5ba-2c57-4b2b-e942-c5b1af109b64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Exercise: Scale cvX and testX as well\n",
        "\n",
        "cvX = scaler.transform(cvX)\n",
        "testX = scaler.transform(testX)\n",
        "\n",
        "\n",
        "#New means and variances\n",
        "#Note that this won't be all zeros and ones like in the previous case. Who wants to explain why?\n",
        "print(\"Variance of cv: \", cvX.var(axis=0))\n",
        "print(\"Mean of cv: \", cvX.mean(axis=0))\n",
        "print(\"Variance of test: \", testX.var(axis=0))\n",
        "print(\"Mean of test: \", testX.mean(axis=0))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Variance of cv:  [2.05687126 0.76169699 0.76169699 0.92443139]\n",
            "Mean of cv:  [ 0.2221881  -0.08922958 -0.08922958 -0.06429328]\n",
            "Variance of test:  [0.92852213 1.23570883 1.23570883 1.21523322]\n",
            "Mean of test:  [0.06930809 0.06744037 0.06744037 0.08487368]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNdqQgziA-eP"
      },
      "source": [
        "Its now time to decide the number of hidden layers and the number of neurons in each layer of our neural network. For now, we just create two hidden layers with 5 and 3 neurons respectively. This is an arbitrary choice. Later, we will see how to choose this appropriately. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHUcTPVeA8hB"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import metrics\n",
        "\n",
        "# Initialize a neural network as described above\n",
        "neural_net = MLPClassifier(hidden_layer_sizes=(5, 3), random_state=1,max_iter=1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "265VWe8jOqzw",
        "outputId": "f7d0ece8-2a77-4c11-c0bb-5ab00ff004dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Fit the model with the training data\n",
        "neural_net.fit(trainX,trainY)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(5, 3), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=1000,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=1, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGrRLBmlO25t",
        "outputId": "7a493188-9d75-46aa-94e5-11ce9f243dc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Exercise:\n",
        "# 1. Predict on the training data\n",
        "# 2. Calculate the training accuracy\n",
        "trainY_pred = neural_net.predict(trainX)\n",
        "acc = metrics.accuracy_score(trainY, trainY_pred, normalize=True)\n",
        "print('Neural Network:\\t-- train acc %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural Network:\t-- train acc 0.799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0dBYFHdO3uN",
        "outputId": "599c78fa-adb6-4e4b-cdc2-54a9692f306b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Exercise:\n",
        "# 1. Predict on the test data\n",
        "# 2. Calculate the test accuracy\n",
        "testY_pred = neural_net.predict(testX)\n",
        "acc = metrics.accuracy_score(testY, testY_pred, normalize=True)\n",
        "print('Neural Network:\\t-- test acc %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neural Network:\t-- test acc 0.740\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0dnrb7JOPtM"
      },
      "source": [
        "What is a MLP classifier? Who wants to explain why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vy82Gz2hIbNE"
      },
      "source": [
        "Okay, so we have a neural network up and running! Is that all? Not quite.. Ideally, we should play around with the inputs to the neural network function to determine what the optimal result should be. These inputs/variables are called the 'hyperparameters'. We could vary the number of layers, the number of neurons in each layer, the learning rate, the type of gradient descent algorithm, the maximum number of iterations and so on. To decide the optimal values, we make use of the cross validation set.\n",
        "\n",
        "Play around with these parameters to see what combination of hyperparamaters gives the highest cross validation accuracy! Then train this model on the training data and look at its accuracy on the test data. This documentation describes the various hyperparameters: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHd4DMpqIX4D",
        "outputId": "3a940bb1-4fa6-49c8-d0bd-56f1e67e0075",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "neural_net = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(7,), random_state=1,max_iter=1000)\n",
        "neural_net.fit(trainX, trainY)\n",
        "cv_pred = neural_net.predict(cvX)\n",
        "acc = metrics.accuracy_score(cv_pred, cvY, normalize=True)\n",
        "print('Cross validation accuracy: %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cross validation accuracy: 0.860\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb4StqDHK6GO"
      },
      "source": [
        "Having found the hyperparameters that give the best accuracy, we use these hyperparameters to train our model and evaluate its performance on the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyXGM2xSK5AM",
        "outputId": "8b5449db-f3db-4ec1-95da-6f0e28f0fb25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "neural_net.fit(trainX,trainY)\n",
        "\n",
        "train_pred = neural_net.predict(trainX)\n",
        "acc = metrics.accuracy_score(train_pred, trainY, normalize=True)\n",
        "print('Training accuracy: %.3f' % acc)\n",
        "\n",
        "test_pred = neural_net.predict(testX)\n",
        "acc = metrics.accuracy_score(test_pred, testY, normalize=True)\n",
        "print('Testing accuracy: %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy: 0.819\n",
            "Testing accuracy: 0.713\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bsnWdjDCLuof"
      },
      "source": [
        "Of course, it's a pain to manually tune the hyperparamters to figure out what the best combination is. This is why we search across a range of possibilities for each hyperparameter and choose the best parameters. An example is given below, but feel free to search amonst more hyperparaters! Of course, we need to keep in mind that the more combinations we introduce, the longer it will take for the model to run. It is always helpful to look at the documentation of the model to figure out which hyperparameters you want to play around with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrjC99NkTCEl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNDd04ZlK3ky",
        "outputId": "43a99268-4228-4238-ddc2-dc774da32bee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "best_acc = -1\n",
        "opt_max_layers = None\n",
        "opt_max_neurons = None\n",
        "opt_lr = None\n",
        "opt_solver = None\n",
        "\n",
        "max_layers = 3\n",
        "max_neurons_per_layer = 6\n",
        "learning_rates = [0.01,0.001,0.0001]\n",
        "solvers = ['adam','lbfgs']\n",
        "\n",
        "import progressbar\n",
        "\n",
        "for i in progressbar.progressbar(range(1,max_layers+1)):\n",
        "  for j in range(2,max_neurons_per_layer):\n",
        "    for lr in learning_rates:\n",
        "      for solver in solvers:\n",
        "        neural_net = MLPClassifier(solver=solver, alpha=lr, hidden_layer_sizes=(j,)*i, random_state=1,max_iter=1000)\n",
        "        neural_net.fit(trainX,trainY) # train model based on training data\n",
        "        cv_pred = neural_net.predict(cvX) # evaluate on cv\n",
        "        acc = metrics.accuracy_score(cv_pred, cvY, normalize=True)\n",
        "        if acc > best_acc:\n",
        "          best_acc = acc\n",
        "          opt_max_layers = i\n",
        "          opt_max_neurons = j\n",
        "          opt_lr = lr\n",
        "          opt_solver = solver\n",
        "print(opt_max_layers, opt_max_neurons, opt_lr, opt_solver)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (3 of 3) |##########################| Elapsed Time: 0:00:24 Time:  0:00:24\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1 2 0.01 lbfgs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STus31lMP7my",
        "outputId": "38f26b07-afdf-47a9-dc51-1547d7a56c02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(opt_max_layers, opt_max_neurons, opt_lr, opt_solver)\n",
        "neural_net = MLPClassifier(solver=opt_solver, alpha=opt_lr, hidden_layer_sizes=(opt_max_neurons,)*opt_max_layers, random_state=1,max_iter=500)\n",
        "neural_net.fit(trainX,trainY)\n",
        "\n",
        "train_pred = neural_net.predict(trainX)\n",
        "acc = metrics.accuracy_score(train_pred, trainY, normalize=True)\n",
        "print('Training accuracy: %.3f' % acc)\n",
        "\n",
        "test_pred = neural_net.predict(testX)\n",
        "acc = metrics.accuracy_score(test_pred, testY, normalize=True)\n",
        "print('Testing accuracy: %.3f' % acc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 2 0.01 lbfgs\n",
            "Training accuracy: 0.819\n",
            "Testing accuracy: 0.727\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPvnRzKqNpkJ"
      },
      "source": [
        "Once you're comfortable with this, you may want to search for 'GridSearchCV' that allows you to perform hyperparameter tuning without writing the for loops yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRCY-twzSHgU"
      },
      "source": [
        "Congrats! You've just built your first neural network and tuned its hyperparameters as well. You can see that the test accuracy after the hypertuning is higher that just any guess."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qcb8awWBNDjJ"
      },
      "source": [
        "Thanks for coming out today! We would be super grateful if you could fill out our feedback for here: [tinyurl.com/applymlfeedbackthree](https://tinyurl.com/applymlfeedbackthree)"
      ]
    }
  ]
}